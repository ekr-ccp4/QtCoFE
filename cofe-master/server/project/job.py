#!/usr/bin/python

#  ------------------------------------------------------------------
#    Job class. Jobs are actual directories that host job data. Job
#  class contains all relevant metadata.
#  ------------------------------------------------------------------

import os
from   project import datamodel
from   varut   import gitut, jsonut, utils, defs

class Job(jsonut.jObject):


    def __init__ ( self,json_str="" ):
        super(Job,self).__init__(json_str)
        if json_str:
            self.__data_from_json()
        return


    def __data_from_json ( self ):

        for i in range(len(self.input)):
            for j in range(len(self.input[i])):
                json_str = self.input[i][j]
                jObj     = jsonut.jObject(json_str)
                self.input[i][j] = datamodel.get_data ( jObj.type,json_str )

        for i in range(len(self.data)):
            for j in range(len(self.data[i])):
                json_str = self.data[i][j]
                jObj     = jsonut.jObject(json_str)
                self.data[i][j] = datamodel.get_data ( jObj.type,json_str )

        return


    def set_minimal_data ( self,name,desc,type,parentId,jobId ):
        self.name     = name   # if empty then taken from task
        self.desc     = desc   # if empty then taken from task
        self.type     = type   # task type
        self.parentId = parentId
        self.id       = jobId
        self.status   = defs.job_idle()
                               # 0   :  idle, haven't run
                               # 100 :  starting
                               # 1000:  running
                               # 2000:  idle, job's done

        self.jobs     = []     # child job(s)

        self.input    = []     # list of job input data. This list
                               # has the same structure as data[]
                               # (see below). In case of 'dataimport'
                               # job, the list will contain the actual
                               # data types, in all other cases it will
                               # contain 'reference' dara classes

        self.data     = []     # data (represented by metadata)
                               # generated by the job
        # all data is arranged in lists like in the following:
        # data = [[data11,data12,...],
        #         [data21,data22,...],
        #         ....................
        #        ]
        # each enclosed list corresponds to a particular data type.

        self.expanded = True
        return


    def add_input ( self,data_class ):

        for i in range(len(self.input)):
            if self.input[i][0].type == data_class.type:
                self.input[i].append ( data_class )
                return

        self.input.append ( [data_class] )
        return


    def set_input ( self,data_class_list ):

        for i in range(len(self.input)):
            if self.input[i][0].type == data_class_list[0].type:
                self.input[i] = data_class_list
                return

        self.input.append ( data_class_list )
        return


    def add_data ( self,data_class ):

        for i in range(len(self.data)):
            if self.data[i][0].type == data_class.type:
                self.data[i].append ( data_class )
                return

        self.data.append ( [data_class] )
        return


    def set_data ( self,data_class_list ):

        for i in range(len(self.data)):
            if self.data[i][0].type == data_class_list[0].type:
                self.data[i] = data_class_list
                return

        self.data.append ( data_class_list )
        return


    def to_JSON(self):   # this function is needed because Qt JSON
                         # functions do not handle arrays of objects
                         # properly

        # remember input and data arrays in local variables and
        # convert them into json strings in-place

        input1 = []
        for i in range(len(self.input)):
            input1i = []
            for j in range(len(self.input[i])):
                input1i.append ( self.input[i][j] )
                self.input[i][j] = self.input[i][j].to_JSON()
            input1.append ( input1i )

        data1 = []
        for i in range(len(self.data)):
            data1i = []
            for j in range(len(self.data[i])):
                data1i.append ( self.data[i][j] )
                self.data[i][j] = self.data[i][j].to_JSON()
            data1.append ( data1i )

        #  produce json string for the whole job
        json_str = super(Job,self).to_JSON()

        #  restore data arrays
        self.input = input1
        self.data  = data1

        return json_str


    def read ( self,project_repo_dir,jobID ):
    #  Lock repository before calling this function. Note that this
    #  function does the actual checkout from repository, therefore:
    #     - avoid redundant calls by all measures
    #     - use it for getting data from jobs

        if not os.path.isdir(project_repo_dir):
            return utils.make_return ( "","no_project_repo_dir",
                                  "Project data repository not found" )

        result = gitut.checkout ( project_repo_dir,[] )
        if result.result != "OK":
            return result

        super(Job,self).read_json ( open (
            os.path.join ( project_repo_dir,"job." + str(jobID),
                                        defs.job_data_name())).read() )
        self.__data_from_json()

        return utils.make_return ( "","OK","OK" )


    def read_from_dir ( self,job_dir ):
    #  This function should be used only in course of internal
    #  operations when repository checkout may be bypassed

        super(Job,self).read_json ( open (
            os.path.join ( job_dir,defs.job_data_name())).read() )
        self.__data_from_json()

        return utils.make_return ( "","OK","OK" )


    def write ( self,project_repo_dir ):
        file = open ( os.path.join(project_repo_dir,"job." + str(self.id),
                               defs.job_data_name()),"w" )
        file.write ( self.to_JSON() )
        file.close ()
        return


    def write_to_dir ( self,job_dir ):
        file = open ( os.path.join(job_dir,defs.job_data_name()),"w" )
        file.write ( self.to_JSON() )
        file.close ()
        return


def get_projected_data ( projected_data,project_repo_dir,jobId ):

    J = Job()
    J.read ( project_repo_dir,jobId )
    for i in range(0,len(J.data)):
        projected = False
        if (len(J.data[i]))>0:
            typei     = J.data[i][0].type
            for j in range(len(projected_data)):
                if projected_data[j][0].type == typei:
                    projected = True
            if not projected:
                projected_data.append ( J.data[i] )

    if J.parentId > 0:
        get_projected_data ( projected_data,project_repo_dir,J.parentId )

    return


def sort_data ( data ):
    for i in range(0,len(data)-1):
        d = data[i]
        for j in range(i+1,len(data)):
            if data[j][0].order < d[0].order:
                d       = data[i]
                data[i] = data[j]
                data[j] = d
    return


def set_dnames ( data ):
    datacnt = 1
    for i in range(0,len(data)):
        for j in range(0,len(data[i])):
            data[i][j].makeDName ( datacnt )
            datacnt += 1
    return


#
#  ------------------------------------------------------------------
#   Tests
#  ------------------------------------------------------------------
#

if __name__ == "__main__":
    import sys
    from varut import jsonut
    from   dtypes import sequence, hkl

    J = Job()
    J.set_minimal_data ( "job-name","job-desc","job-type",0,1 )
    print "\nJob:\n\n" + J.to_JSON()

    seq      = sequence.DType()
    seq.file = "sequence.seq"
    J.set_data ( [seq] )
    print "\nJob:\n\n" + J.to_JSON()

    hkl_data  = hkl.DType()
    hkl_data.file = "merged.mtz"
    J.add_data ( hkl_data )
    print "\nJob:\n\n" + J.to_JSON()

    json_str = J.to_JSON()
    print "----------------------------------------------------------"

    B = Job(json_str);
    print "\nB:\n\n" + B.to_JSON()
    print B.data[0][0].type
    print B.data[1][0].type


    sys.exit(0)
